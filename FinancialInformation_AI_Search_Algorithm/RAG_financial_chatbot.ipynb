{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 재정정보 AI 검색 알고리즘 경진대회"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import random\n",
    "from transformers import set_seed\n",
    "\n",
    "# 시드 설정\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import pdfplumber\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    AwqConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain.document_loaders import PDFPlumberLoader, PyMuPDFLoader, PyPDFLoader, UnstructuredPDFLoader\n",
    "from peft import PeftModel, LoraConfig, PeftConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config():\n",
    "    # LoRA 설정 정의\n",
    "    config = LoraConfig(\n",
    "        r=32,  # LoRA rank\n",
    "        lora_alpha=64,  # LoRA scaling factor\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # 적용할 모듈\n",
    "        lora_dropout=0.2,  # 드롭아웃 확률\n",
    "        bias=\"lora_only\",  # LoRA에서 bias 처리 방법 (none, all, lora_only)\n",
    "        task_type=\"SEQ2SEQ_LM\"  # 작업 유형 (예: \"SEQ2SEQ_LM\", \"CAUSAL_LM\")\n",
    "    )\n",
    "\n",
    "    # Config를 JSON 파일로 저장\n",
    "    config.save_pretrained(\"./persona\")\n",
    "\n",
    "create_peft_config()\n",
    "\n",
    "# meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        # 모델별 설정 딕셔너리\n",
    "        self.model_configs = {\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\":\n",
    "            {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"google/gemma-2-27b-it\": {\n",
    "                \"quantization_config\": BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "                ),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 선택된 모델\n",
    "        self.llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # \"google/gemma-2-27b-it\"\n",
    "        self.llm_model_config = self.model_configs[self.llm_model]\n",
    "        self.llm_peft = True\n",
    "        self.llm_peft_checkpoint = \"./persona\"\n",
    "        \n",
    "        # \n",
    "        self.embed_models = [\"intfloat/multilingual-e5-base\", \"intfloat/multilingual-e5-large-instruct\", \"intfloat/multilingual-e5-large\"]\n",
    "        self.embed_model = self.embed_models[2]\n",
    "        \n",
    "        # \n",
    "        self.pdf_loader = \"pdfplumber\"\n",
    "        \n",
    "        self.base_directory = \"../../data\"\n",
    "        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n",
    "        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n",
    "        self.chunk_size = 512\n",
    "        self.chunk_overlap = 32\n",
    "        \n",
    "        self.ensemble = True\n",
    "        self.bm25_w = 0.5\n",
    "        self.faiss_w = 0.5\n",
    "        \n",
    "        self.is_submit = True\n",
    "        \n",
    "        self.eval_sum_mode = False\n",
    "        \n",
    "        self.output_dir = \"test_results\"\n",
    "        self.output_csv_file = f\"{self.llm_model.split('/')[1]}_{self.embed_model.split('/')[1]}_pdf{self.pdf_loader}_chks{self.chunk_size}_chkovp{self.chunk_overlap}_bm25{self.bm25_w}_faiss{self.faiss_w}_mix_submission.csv\"\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "    \n",
    "        \n",
    "args=Opt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.llm_model)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.llm_model,\n",
    "        quantization_config=args.llm_model_config['quantization_config'],\n",
    "        torch_dtype=args.llm_model_config['torch_dtype'],\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # PEFT 설정이 적용된 모델 로드\n",
    "    if args.llm_peft:\n",
    "        peft_config = PeftConfig.from_pretrained(args.llm_peft_checkpoint)\n",
    "        model = PeftModel(model, peft_config)\n",
    "    \n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=args.llm_model_config['max_token'],\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# peft 로 fine tuning을 수행하기에는 아직 라이브러리에서 \"text generation\"을 지원하지 않아서 소용이 없었다.\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_path(path):\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def extract_text_with_pdfplumber(file_path):\n",
    "    \"\"\"pdfplumber를 사용하여 PDF의 텍스트를 추출\"\"\"\n",
    "    full_text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                full_text += text + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    \"\"\"PDF에서 텍스트를 추출하고, Markdown 스타일로 처리한 후 분할\"\"\"\n",
    "    # pdfplumber로 텍스트 추출\n",
    "    md_text = extract_text_with_pdfplumber(file_path)\n",
    "    \n",
    "    # Markdown 헤더 기준으로 텍스트를 분할\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    md_chunks = md_header_splitter.split_text(md_text)\n",
    "\n",
    "    # 텍스트를 chunk 단위로 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(md_chunks)\n",
    "    return splits\n",
    "\n",
    "def create_vector_db(chunks, model_path):\n",
    "    \"\"\"FAISS 벡터 DB 생성\"\"\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    \n",
    "    # 메모리 캐시 비우기\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 PDF명을 키로 해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "\n",
    "        db = create_vector_db(chunks, model_path=args.embed_model)\n",
    "        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
    "        faiss_retriever = db.as_retriever()\n",
    " \n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n",
    "            weights=[args.bm25_w, args.faiss_w],\n",
    "            search_type=\"mmr\",\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "            'db': db,\n",
    "            'retriever': retriever\n",
    "        }\n",
    "        \n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = args.base_directory\n",
    "train_df = pd.read_csv(args.train_csv_path)\n",
    "test_df = pd.read_csv(args.test_csv_path)\n",
    "train_pdf_databases = None\n",
    "test_pdf_databases = None\n",
    "if args.is_submit:\n",
    "    test_pdf_databases = process_pdfs_from_dataframe(test_df, base_directory)\n",
    "else:\n",
    "    train_pdf_databases = process_pdfs_from_dataframe(train_df, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df if args.is_submit else train_df\n",
    "pdf_databases = test_pdf_databases if args.is_submit else train_pdf_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Retriever QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "\n",
    "    # 정규화된 키로 데이터베이스 검색\n",
    "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "    # 주어진 질문에만 답변하세요. 문장으로 답변해주세요. 답변할 때 질문의 주어를 써주세요.\n",
    "    # 질문의 핵심만 파악하여 간결하게 1-2문장으로 답변하고, 불필요한 설명은 피하며 요구된 정보만 제공하세요.\n",
    "    template = \"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    ### 질문:\n",
    "    {question}\n",
    "    \n",
    "    질문의 핵심만 파악하여 간결하게 답변하고, 불필요한 설명은 피하며 요구된 정보만 검토 후 제공하세요.\n",
    "    특히 금액 단위를 검토하세요.\n",
    "    \n",
    "    ### 답변:\n",
    "    <|eot_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template) \n",
    "   \n",
    "    # RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 답변 추론\n",
    "    print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(\"../../data/sample_submission.csv\")\n",
    "\n",
    "# 생성된 답변에서 앞뒤의 공백 및 줄바꿈 제거 후 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'].strip() for item in results]  # strip()으로 앞뒤 공백 제거\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")  # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [주의]\n",
    "\n",
    "display(submit_df.head(3))\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(\"lastpang_1.csv\", encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.691 -> pdfplumber + multilingual-base + llama-3.1-8b-Instruct\n",
    "# 0.706 -> pdf split + pdfplumber + multilingual-e5-base + llama-3.1-8b-Instruct\n",
    "# 0.712 -> pdf split + pdfplumber + multilingual-e5-large + llama-3.1-8b-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
