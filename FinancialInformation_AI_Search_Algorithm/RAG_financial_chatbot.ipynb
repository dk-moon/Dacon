{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# 재정정보 AI 검색 알고리즘 경진대회\n",
    "- 일시 : 2024.07.29 ~ 2024.08.23 09:59\n",
    "- 순위(전체 360팀, 참가자 수 1,001명)\n",
    "    - Public : 21 등, score : 0.71238\n",
    "    - Private : 24 등, score : 0.68749\n",
    "- https://dacon.io/competitions/official/236295/overview/description\n",
    "- 주최 : 한국재정정보원, 기획재정부\n",
    "- 규칙\n",
    "    - 외부 데이터 사용 금지\n",
    "    - 학습 데이터 증강 가능 : 제공된 훈련 데이터를 증강할 수 있지만, ChatGPT, Claude 등과 같은 모델의 **코드와 가중치 파일이 공개되지 않은 LLM(또는 사전학습 모델)**은 사용 불가. (데이터 전처리에도 동일한 규칙 적용)\n",
    "    - 공식 공개 사전 학습 모델 사용 가능 : 가중치 파일이 공식적으로 공개되고 사용에 법적 제약이 없는 사전 학습 모델은 사용 가능\n",
    "    - 유료 LLM 모델 API 사용 금지\n",
    "    - 허용 기법 : 순수 프롬프팅, RAG, 파인튜닝\n",
    "\n",
    "# 정리\n",
    "- 열린재정의 중앙정부 재정 정보 검색 및 제공 편의성과 활용도를 높이는 Retrieval-Augmented Generation(RAG) 기반 Chatbot 개발\n",
    "- '재정 정보 질의 응답 데이터셋'과 재정 보고서, 예산 설명자료, 기획재정부 보도자료 등 다양한 재정 관련 텍스트 데이터를 활용해 주어진 질문에 대해 정확도가 높은 응답을 제시하는 자연어처리 알고리즘 개발\n",
    "\n",
    "# Our Project\n",
    "\n",
    "## Data Load\n",
    "- PDF 데이터를 불러오기 위해 pdfplumber 라이브러리 활용\n",
    "- 일부 PDF 문서가 가로용지 양식으로 2페이지가 한 개의 페이지로 묶여있어서, pdfplumber 사용시 가로로 텍스트 데이터 추출 과정에서 데이터의 손상이 발생이되어, 이를 가로 용지의 부분을 반으로 나누어 한 개의 페이지로 PDF 파일 수정\n",
    "- fitz 사용시 최대 score : 0.62(public)\n",
    "- pdfplumber 사용시 최대 score : 0.69(public)\n",
    "- pdf 분할 + pdfplumber 사용시 최대 score : 0.71\n",
    "- 아쉬운 점\n",
    "    - pdf 에서 마크다운 형식으로 수작업으로 변환하여 그림 및 표의 양식을 자연어 텍스트 양식으로 바꾸었지만, 제대로 활용할 시간이 부족했다.\n",
    "\n",
    "## Embedding\n",
    "- 사용 모델 : intfloat/multilingual-e5-large\n",
    "- 처음엔, 임베딩 모델을 KoBERT, RoBERTa 등 다양한 방법을 사용하였으나 multilingual-e5-base 보다 성능이 개선되지 않았다.\n",
    "- GPU를 사용하여 임베딩을 수행하다 보니, 임베딩 모델의 크기에 따라 GPU 메모리가 많이 할당이 되어 일반적인 Local 환경에서 구동의 어려움이 있었다.\n",
    "    - 이를, 임베딩 모델을 **CPU**로 수행하도록 변경하니 multilingual-e5-large 도 충분히 사용가능하였다.(단, Local의 RAM 크기가 32GB 이상. 임베딩시 약 25GB 사용 - 임베딩 데이터가 많아질 수록 메모리 사용량 높아진다)\n",
    "- 아쉬운 점\n",
    "    - CPU로 변경하여 사용하면 리소스 문제를 어느정도 해결할 수 있다는 것을 뒤늦게 알게되어 chunk 조절 등 다양한 실험을 수행할 시간 확보를 충분히 하지 못했다.\n",
    "    - \"bespin-global/klue-sroberta-base-continue-learning-by-mnr\", \"Alibaba-NLP/gte-multilingual-base\", \"BAAI/bge-multilingual-gemma2\" 등 더 다양한 모델을 사용하여 성능을 비교하지 못했다.\n",
    "\n",
    "## VectorDB\n",
    "- FAISS 사용\n",
    "- 아쉬운점\n",
    "    - FAISS가 현재 데이콘 기간에서 빠르게 결과를 도출하는데 있어서 편리한 점이 많아서 다른 VectorDB를 적용은 따로 수행하지 않았다.\n",
    "\n",
    "## LLM\n",
    "- 사용 모델 : meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "- Why : 성능이 준수하고 그나마 가벼운 모델이기에 적용\n",
    "- 적용해 본 모델 : gemma-2-8b, gemma-2-21b(두 모델 모두 llama-3.1-8b보다 성능이 개선되지 않았다.)\n",
    "- 아쉬운점\n",
    "    - \"meta-llama/Meta-Llama-3.1-70B-Instruct\"도 적용해 보고 싶었지만, 해당 모델과 더 큰 모델의 경우 고사양의 GPU 리소스를 요구하여 Tesla-V100으로도 정상적으로 수행에 어려움이 존재하였다.\n",
    "    - RAG 기법이 아닌, Fine-tuning을 제대로 시도해보지 못 했던 점이 아직 공부해야 될 점인 것 같다.(결과가 어떻든 성능을 한번이라도 내본 경험을 했어야 했다. RAG에서 성능을 급격히 향상시키다 보니 이 부분을 소홀했던 것 같다.)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Project Code"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Import Libraries"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import gc\n",
    "import torch\n",
    "import torchvision\n",
    "import json\n",
    "import unicodedata\n",
    "\n",
    "import random\n",
    "from transformers import set_seed\n",
    "\n",
    "# 시드 설정\n",
    "seed = 42\n",
    "random.seed(seed)\n",
    "torch.manual_seed(seed)\n",
    "if torch.cuda.is_available():\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "set_seed(seed)\n",
    "\n",
    "import pandas as pd\n",
    "from tqdm import tqdm\n",
    "import pymupdf\n",
    "import pymupdf4llm\n",
    "import pdfplumber\n",
    "from collections import Counter\n",
    "\n",
    "from transformers import (\n",
    "    AutoTokenizer,\n",
    "    AutoModelForCausalLM,\n",
    "    pipeline,\n",
    "    BitsAndBytesConfig,\n",
    "    AwqConfig\n",
    ")\n",
    "from accelerate import Accelerator\n",
    "\n",
    "# Langchain 관련\n",
    "from langchain.llms import HuggingFacePipeline\n",
    "from langchain_huggingface import HuggingFaceEmbeddings\n",
    "from langchain.vectorstores import FAISS\n",
    "from langchain.schema import Document\n",
    "from langchain.text_splitter import RecursiveCharacterTextSplitter, MarkdownTextSplitter, MarkdownHeaderTextSplitter\n",
    "from langchain.prompts import PromptTemplate, ChatPromptTemplate\n",
    "from langchain.schema.runnable import RunnablePassthrough\n",
    "from langchain.schema.output_parser import StrOutputParser\n",
    "from langchain_teddynote.retrievers import KiwiBM25Retriever\n",
    "from langchain.retrievers import EnsembleRetriever, MultiQueryRetriever\n",
    "from langchain_community.chat_models import ChatOllama\n",
    "\n",
    "from langchain.document_loaders import PDFPlumberLoader, PyMuPDFLoader, PyPDFLoader, UnstructuredPDFLoader\n",
    "from peft import PeftModel, LoraConfig, PeftConfig"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Set Config"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def create_peft_config():\n",
    "    # LoRA 설정 정의\n",
    "    config = LoraConfig(\n",
    "        r=32,  # LoRA rank\n",
    "        lora_alpha=64,  # LoRA scaling factor\n",
    "        target_modules=[\"q_proj\", \"v_proj\", \"k_proj\"],  # 적용할 모듈\n",
    "        lora_dropout=0.2,  # 드롭아웃 확률\n",
    "        bias=\"lora_only\",  # LoRA에서 bias 처리 방법 (none, all, lora_only)\n",
    "        task_type=\"SEQ2SEQ_LM\"  # 작업 유형 (예: \"SEQ2SEQ_LM\", \"CAUSAL_LM\")\n",
    "    )\n",
    "\n",
    "    # Config를 JSON 파일로 저장\n",
    "    config.save_pretrained(\"./persona\")\n",
    "\n",
    "create_peft_config()\n",
    "\n",
    "# meta-llama/Meta-Llama-3.1-8B-Instruct\n",
    "class Opt:\n",
    "    def __init__(self):\n",
    "        # 모델별 설정 딕셔너리\n",
    "        self.model_configs = {\n",
    "            \"meta-llama/Meta-Llama-3.1-8B-Instruct\":\n",
    "            {\n",
    "                \"quantization_config\": None,\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256,\n",
    "            },\n",
    "            \"google/gemma-2-27b-it\": {\n",
    "                \"quantization_config\": BitsAndBytesConfig(\n",
    "                    load_in_4bit=True,\n",
    "                    bnb_4bit_use_double_quant=True,\n",
    "                    bnb_4bit_quant_type=\"nf4\",\n",
    "                    bnb_4bit_compute_dtype=torch.bfloat16\n",
    "                ),\n",
    "                \"torch_dtype\": \"auto\",\n",
    "                \"max_token\": 256\n",
    "            }\n",
    "        }\n",
    "        \n",
    "        # 선택된 모델\n",
    "        self.llm_model = \"meta-llama/Meta-Llama-3.1-8B-Instruct\" # \"google/gemma-2-27b-it\"\n",
    "        self.llm_model_config = self.model_configs[self.llm_model]\n",
    "        self.llm_peft = True\n",
    "        self.llm_peft_checkpoint = \"./persona\"\n",
    "        \n",
    "        # \n",
    "        self.embed_models = [\"intfloat/multilingual-e5-base\", \"intfloat/multilingual-e5-large-instruct\", \"intfloat/multilingual-e5-large\"]\n",
    "        self.embed_model = self.embed_models[2]\n",
    "        \n",
    "        # \n",
    "        self.pdf_loader = \"pdfplumber\"\n",
    "        \n",
    "        self.base_directory = \"../../data\"\n",
    "        self.train_csv_path = os.path.join(self.base_directory, \"train.csv\")\n",
    "        self.test_csv_path = os.path.join(self.base_directory, \"test.csv\")\n",
    "        self.chunk_size = 512\n",
    "        self.chunk_overlap = 32\n",
    "        \n",
    "        self.ensemble = True\n",
    "        self.bm25_w = 0.5\n",
    "        self.faiss_w = 0.5\n",
    "        \n",
    "        self.is_submit = True\n",
    "        \n",
    "        self.eval_sum_mode = False\n",
    "        \n",
    "        self.output_dir = \"test_results\"\n",
    "        self.output_csv_file = f\"{self.llm_model.split('/')[1]}_{self.embed_model.split('/')[1]}_pdf{self.pdf_loader}_chks{self.chunk_size}_chkovp{self.chunk_overlap}_bm25{self.bm25_w}_faiss{self.faiss_w}_mix_submission.csv\"\n",
    "        \n",
    "        os.makedirs(self.output_dir, exist_ok=True)\n",
    "        \n",
    "    def to_json(self):\n",
    "        return json.dumps(self.__dict__)\n",
    "    \n",
    "        \n",
    "args=Opt()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### LLM Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def setup_llm_pipeline():\n",
    "    tokenizer = AutoTokenizer.from_pretrained(args.llm_model)\n",
    "    tokenizer.use_default_system_prompt = False\n",
    "    \n",
    "    model = AutoModelForCausalLM.from_pretrained(\n",
    "        args.llm_model,\n",
    "        quantization_config=args.llm_model_config['quantization_config'],\n",
    "        torch_dtype=args.llm_model_config['torch_dtype'],\n",
    "        device_map=\"auto\",\n",
    "        trust_remote_code=True\n",
    "    )\n",
    "\n",
    "    # PEFT 설정이 적용된 모델 로드\n",
    "    if args.llm_peft:\n",
    "        peft_config = PeftConfig.from_pretrained(args.llm_peft_checkpoint)\n",
    "        model = PeftModel(model, peft_config)\n",
    "    \n",
    "    text_generation_pipeline = pipeline(\n",
    "        model=model,\n",
    "        tokenizer=tokenizer,\n",
    "        task=\"text-generation\",\n",
    "        return_full_text=False,\n",
    "        max_new_tokens=args.llm_model_config['max_token'],\n",
    "    )\n",
    "\n",
    "    return HuggingFacePipeline(pipeline=text_generation_pipeline)\n",
    "\n",
    "# peft 로 fine tuning을 수행하기에는 아직 라이브러리에서 \"text generation\"을 지원하지 않아서 소용이 없었다.\n",
    "llm = setup_llm_pipeline()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data Prepare"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_path(path):\n",
    "    return unicodedata.normalize('NFC', path)\n",
    "\n",
    "def format_docs(docs):\n",
    "    return \"\\n\".join([doc.page_content for doc in docs])\n",
    "\n",
    "def extract_text_with_pdfplumber(file_path):\n",
    "    \"\"\"pdfplumber를 사용하여 PDF의 텍스트를 추출\"\"\"\n",
    "    full_text = \"\"\n",
    "    with pdfplumber.open(file_path) as pdf:\n",
    "        for page in pdf.pages:\n",
    "            text = page.extract_text()\n",
    "            if text:\n",
    "                full_text += text + \"\\n\"\n",
    "    return full_text\n",
    "\n",
    "def process_pdf(file_path):\n",
    "    \"\"\"PDF에서 텍스트를 추출하고, Markdown 스타일로 처리한 후 분할\"\"\"\n",
    "    # pdfplumber로 텍스트 추출\n",
    "    md_text = extract_text_with_pdfplumber(file_path)\n",
    "    \n",
    "    # Markdown 헤더 기준으로 텍스트를 분할\n",
    "    headers_to_split_on = [\n",
    "        (\"#\", \"Header 1\"),\n",
    "        (\"##\", \"Header 2\"),\n",
    "        (\"###\", \"Header 3\"),\n",
    "    ]\n",
    "\n",
    "    md_header_splitter = MarkdownHeaderTextSplitter(headers_to_split_on=headers_to_split_on, strip_headers=False)\n",
    "    md_chunks = md_header_splitter.split_text(md_text)\n",
    "\n",
    "    # 텍스트를 chunk 단위로 분할\n",
    "    text_splitter = RecursiveCharacterTextSplitter(\n",
    "        chunk_size=args.chunk_size, chunk_overlap=args.chunk_overlap\n",
    "    )\n",
    "\n",
    "    splits = text_splitter.split_documents(md_chunks)\n",
    "    return splits\n",
    "\n",
    "def create_vector_db(chunks, model_path):\n",
    "    \"\"\"FAISS 벡터 DB 생성\"\"\"\n",
    "    model_kwargs = {'device': 'cpu'}\n",
    "    encode_kwargs = {'normalize_embeddings': True}\n",
    "    embeddings = HuggingFaceEmbeddings(\n",
    "        model_name=model_path,\n",
    "        model_kwargs=model_kwargs,\n",
    "        encode_kwargs=encode_kwargs\n",
    "    )\n",
    "    db = FAISS.from_documents(chunks, embedding=embeddings)\n",
    "    \n",
    "    # 메모리 캐시 비우기\n",
    "    torch.cuda.empty_cache()\n",
    "    gc.collect()\n",
    "    \n",
    "    return db"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def process_pdfs_from_dataframe(df, base_directory):\n",
    "    \"\"\"딕셔너리에 PDF명을 키로 해서 DB, retriever 저장\"\"\"\n",
    "    pdf_databases = {}\n",
    "    unique_paths = df['Source_path'].unique()\n",
    "    \n",
    "    for path in tqdm(unique_paths, desc=\"Processing PDFs\"):\n",
    "        # 경로 정규화 및 절대 경로 생성\n",
    "        normalized_path = normalize_path(path)\n",
    "        full_path = os.path.normpath(os.path.join(base_directory, normalized_path.lstrip('./'))) if not os.path.isabs(normalized_path) else normalized_path\n",
    "        \n",
    "        pdf_title = os.path.splitext(os.path.basename(full_path))[0]\n",
    "        print(f\"Processing {pdf_title}...\")\n",
    "        \n",
    "        # PDF 처리 및 벡터 DB 생성\n",
    "        chunks = process_pdf(full_path)\n",
    "\n",
    "        db = create_vector_db(chunks, model_path=args.embed_model)\n",
    "        kiwi_bm25_retriever = KiwiBM25Retriever.from_documents(chunks)\n",
    "        faiss_retriever = db.as_retriever()\n",
    " \n",
    "        retriever = EnsembleRetriever(\n",
    "            retrievers=[kiwi_bm25_retriever, faiss_retriever],\n",
    "            weights=[args.bm25_w, args.faiss_w],\n",
    "            search_type=\"mmr\",\n",
    "        )\n",
    "\n",
    "        # 결과 저장\n",
    "        pdf_databases[pdf_title] = {\n",
    "            'db': db,\n",
    "            'retriever': retriever\n",
    "        }\n",
    "        \n",
    "    return pdf_databases"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "base_directory = args.base_directory\n",
    "train_df = pd.read_csv(args.train_csv_path)\n",
    "test_df = pd.read_csv(args.test_csv_path)\n",
    "train_pdf_databases = None\n",
    "test_pdf_databases = None\n",
    "if args.is_submit:\n",
    "    test_pdf_databases = process_pdfs_from_dataframe(test_df, base_directory)\n",
    "else:\n",
    "    train_pdf_databases = process_pdfs_from_dataframe(train_df, base_directory)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = test_df if args.is_submit else train_df\n",
    "pdf_databases = test_pdf_databases if args.is_submit else train_pdf_databases"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## RAG Retriever QA"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def normalize_string(s):\n",
    "    \"\"\"유니코드 정규화\"\"\"\n",
    "    return unicodedata.normalize('NFC', s)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 결과를 저장할 리스트 초기화\n",
    "results = []\n",
    "\n",
    "# DataFrame의 각 행에 대해 처리\n",
    "for _, row in tqdm(df.iterrows(), total=len(df), desc=\"Answering Questions\"):\n",
    "    # 소스 문자열 정규화\n",
    "    source = normalize_string(row['Source'])\n",
    "    question = row['Question']\n",
    "\n",
    "    # 정규화된 키로 데이터베이스 검색\n",
    "    normalized_keys = {normalize_string(k): v for k, v in pdf_databases.items()}\n",
    "    retriever = normalized_keys[source]['retriever']\n",
    "\n",
    "    # 주어진 질문에만 답변하세요. 문장으로 답변해주세요. 답변할 때 질문의 주어를 써주세요.\n",
    "    # 질문의 핵심만 파악하여 간결하게 1-2문장으로 답변하고, 불필요한 설명은 피하며 요구된 정보만 제공하세요.\n",
    "    template = \"\"\"\n",
    "    다음 정보를 바탕으로 질문에 답하세요:\n",
    "    {context}\n",
    "\n",
    "    ### 질문:\n",
    "    {question}\n",
    "    \n",
    "    질문의 핵심만 파악하여 간결하게 답변하고, 불필요한 설명은 피하며 요구된 정보만 검토 후 제공하세요.\n",
    "    특히 금액 단위를 검토하세요.\n",
    "    \n",
    "    ### 답변:\n",
    "    <|eot_id|>\n",
    "    \"\"\"\n",
    "    \n",
    "    prompt = PromptTemplate.from_template(template) \n",
    "   \n",
    "    # RAG 체인 정의\n",
    "    rag_chain = (\n",
    "        {\"context\": retriever | format_docs, \"question\": RunnablePassthrough()}\n",
    "        | prompt\n",
    "        | llm\n",
    "        | StrOutputParser()\n",
    "    )\n",
    "\n",
    "    # 답변 추론\n",
    "    print(f\"Question: {question}\")\n",
    "    full_response = rag_chain.invoke(question)\n",
    "    \n",
    "    print(f\"Answer: {full_response}\\n\")\n",
    "\n",
    "    # 결과 저장\n",
    "    results.append({\n",
    "        \"Source\": row['Source'],\n",
    "        \"Source_path\": row['Source_path'],\n",
    "        \"Question\": question,\n",
    "        \"Answer\": full_response\n",
    "    })"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Submission"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 제출용 샘플 파일 로드\n",
    "submit_df = pd.read_csv(\"../../data/sample_submission.csv\")\n",
    "\n",
    "# 생성된 답변에서 앞뒤의 공백 및 줄바꿈 제거 후 제출 DataFrame에 추가\n",
    "submit_df['Answer'] = [item['Answer'].strip() for item in results]  # strip()으로 앞뒤 공백 제거\n",
    "submit_df['Answer'] = submit_df['Answer'].fillna(\"데이콘\")  # 모델에서 빈 값 (NaN) 생성 시 채점에 오류가 날 수 있음 [주의]\n",
    "\n",
    "display(submit_df.head(3))\n",
    "# 결과를 CSV 파일로 저장\n",
    "submit_df.to_csv(\"lastpang_1.csv\", encoding='UTF-8-sig', index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# 0.691 -> pdfplumber + multilingual-base + llama-3.1-8b-Instruct\n",
    "# 0.706 -> pdf split + pdfplumber + multilingual-e5-base + llama-3.1-8b-Instruct\n",
    "# 0.712 -> pdf split + pdfplumber + multilingual-e5-large + llama-3.1-8b-Instruct"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "dacon",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
